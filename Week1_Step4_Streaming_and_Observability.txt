Week 1 â€“ Step 4: Streaming Responses & Observability

ğŸ¯ Objective
Upgrade your AI Chat API from static responses to real-time, streaming outputsâ€”while adding observability tools to measure performance and latency.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ§  Core Goals
1. Implement live token streaming (real-time output like ChatGPT).
2. Add timing and logging for observability.
3. Validate both synchronous and streaming routes.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Step-by-Step Implementation

1ï¸âƒ£ Verify Base Setup
----------------------
Ensure your project is running correctly from Step 3.

Check structure:
ai-chat-api/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.ts
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â””â”€â”€ chat.ts
â”‚   â””â”€â”€ services/
â”‚       â””â”€â”€ openaiService.ts
â”œâ”€â”€ .env
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
2ï¸âƒ£ Update src/services/openaiService.ts
----------------------------------------
Add streaming logic while keeping the existing getAIResponse function.

import OpenAI from "openai";
import dotenv from "dotenv";

dotenv.config();

const client = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Standard non-streaming
export async function getAIResponse(message: string) {
  try {
    const response = await client.chat.completions.create({
      model: "gpt-4o-mini",
      messages: [{ role: "user", content: message }],
    });

    return response.choices[0].message.content ?? "No response received.";
  } catch (error: any) {
    console.error("OpenAI API Error:", error);
    return "âš ï¸ An error occurred while connecting to OpenAI.";
  }
}

// Streaming version
export async function streamAIResponse(message: string, res: any) {
  try {
    const stream = await client.chat.completions.stream({
      model: "gpt-4o-mini",
      messages: [{ role: "user", content: message }],
    });

    res.setHeader("Content-Type", "text/event-stream");
    res.setHeader("Cache-Control", "no-cache");
    res.setHeader("Connection", "keep-alive");

    for await (const chunk of stream) {
      const content = chunk.choices[0]?.delta?.content;
      if (content) res.write(content);
    }

    res.end();
  } catch (error: any) {
    console.error("Streaming error:", error);
    res.status(500).end("Error during streaming");
  }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
3ï¸âƒ£ Update src/routes/chat.ts
-----------------------------
Combine both the standard and streaming routes in one router.

import express from "express";
import { performance } from "perf_hooks";
import { getAIResponse, streamAIResponse } from "../services/openaiService";

const router = express.Router();

// Non-streaming route (returns complete response)
router.post("/", async (req, res) => {
  const { message } = req.body;
  if (!message) return res.status(400).json({ error: "Message required" });

  console.log("ğŸ§  User message:", message);
  const start = performance.now();

  const reply = await getAIResponse(message);

  const end = performance.now();
  console.log(`â± Response time: ${(end - start).toFixed(2)} ms`);

  res.json({ reply });
});

// Streaming route (sends tokens live)
router.post("/stream", async (req, res) => {
  const { message } = req.body;
  if (!message) return res.status(400).end("Message required");

  const start = performance.now();
  await streamAIResponse(message, res);
  const end = performance.now();
  console.log(`â± Stream duration: ${(end - start).toFixed(2)} ms`);
});

export default router;

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
4ï¸âƒ£ Update src/app.ts
---------------------
Ensure your Express app correctly loads the chat router before starting the server.

import express from "express";
import cors from "cors";
import dotenv from "dotenv";
import chatRouter from "./routes/chat"; // âœ… Make sure this is imported at the top

dotenv.config();

const app = express();
const PORT = process.env.PORT || 3000;

app.use(cors());
app.use(express.json());

// âœ… Root test route
app.get("/", (req, res) => {
  res.send("ğŸš€ AI Chat API is running!");
});

// âœ… Mount all chat routes
app.use("/api/chat", chatRouter);

// âœ… Start server
app.listen(PORT, () => {
  console.log(`âœ… Server running on http://localhost:${PORT}`);
});

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
5ï¸âƒ£ Test Everything
-------------------
Run your dev server:
npm run dev

âœ… Test standard mode:
curl -X POST http://localhost:3000/api/chat -H "Content-Type: application/json" -d "{"message":"Write a motivational quote."}"

Expected output:
{"reply":"\"Success is not the destination, but the journey of perseverance and passion that transforms every setback into a stepping stone.\""}  

âœ… Test streaming mode:
curl -N -X POST http://localhost:3000/api/chat/stream -H "Content-Type: application/json" -d "{"message":"Write a one-sentence leadership quote."}"

Expected streaming output (token by token):
"True leadership is not about being in charge, but about inspiring others to rise and excel together."

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
6ï¸âƒ£ Confirm Logging and Observability
--------------------------------------
Console output example:
ğŸ§  User message: Write a motivational quote.
â± Response time: 542.31 ms
â± Stream duration: 779.78 ms

âœ… This confirms observability and performance logging are working.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ¯ Outcome
Your API now supports:
â€¢ Real-time streaming responses
â€¢ Performance timing for every request
â€¢ Clean route structure and modular service layer
â€¢ Both synchronous and streaming AI endpoints

You now have a **production-grade AI backend** â€” exactly what employers expect from an AI Integration Engineer.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
End of Week 1 â€“ Step 4 (Streaming & Observability)
