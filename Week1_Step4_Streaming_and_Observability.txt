Week 1 – Step 4: Streaming Responses & Observability

🎯 Objective
Upgrade your AI Chat API from static responses to real-time, streaming outputs—while adding observability tools to measure performance and latency.

──────────────────────────────────────────────────────────────────────────────
🧠 Core Goals
1. Implement live token streaming (real-time output like ChatGPT).
2. Add timing and logging for observability.
3. Validate both synchronous and streaming routes.

──────────────────────────────────────────────────────────────────────────────
✅ Step-by-Step Implementation

1️⃣ Verify Base Setup
----------------------
Ensure your project is running correctly from Step 3.

Check structure:
ai-chat-api/
├── src/
│   ├── app.ts
│   ├── routes/
│   │   └── chat.ts
│   └── services/
│       └── openaiService.ts
├── .env
├── package.json
├── tsconfig.json

──────────────────────────────────────────────────────────────────────────────
2️⃣ Update src/services/openaiService.ts
----------------------------------------
Add streaming logic while keeping the existing getAIResponse function.

import OpenAI from "openai";
import dotenv from "dotenv";

dotenv.config();

const client = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

// Standard non-streaming
export async function getAIResponse(message: string) {
  try {
    const response = await client.chat.completions.create({
      model: "gpt-4o-mini",
      messages: [{ role: "user", content: message }],
    });

    return response.choices[0].message.content ?? "No response received.";
  } catch (error: any) {
    console.error("OpenAI API Error:", error);
    return "⚠️ An error occurred while connecting to OpenAI.";
  }
}

// Streaming version
export async function streamAIResponse(message: string, res: any) {
  try {
    const stream = await client.chat.completions.stream({
      model: "gpt-4o-mini",
      messages: [{ role: "user", content: message }],
    });

    res.setHeader("Content-Type", "text/event-stream");
    res.setHeader("Cache-Control", "no-cache");
    res.setHeader("Connection", "keep-alive");

    for await (const chunk of stream) {
      const content = chunk.choices[0]?.delta?.content;
      if (content) res.write(content);
    }

    res.end();
  } catch (error: any) {
    console.error("Streaming error:", error);
    res.status(500).end("Error during streaming");
  }
}

──────────────────────────────────────────────────────────────────────────────
3️⃣ Update src/routes/chat.ts
-----------------------------
Combine both the standard and streaming routes in one router.

import express from "express";
import { performance } from "perf_hooks";
import { getAIResponse, streamAIResponse } from "../services/openaiService";

const router = express.Router();

// Non-streaming route (returns complete response)
router.post("/", async (req, res) => {
  const { message } = req.body;
  if (!message) return res.status(400).json({ error: "Message required" });

  console.log("🧠 User message:", message);
  const start = performance.now();

  const reply = await getAIResponse(message);

  const end = performance.now();
  console.log(`⏱ Response time: ${(end - start).toFixed(2)} ms`);

  res.json({ reply });
});

// Streaming route (sends tokens live)
router.post("/stream", async (req, res) => {
  const { message } = req.body;
  if (!message) return res.status(400).end("Message required");

  const start = performance.now();
  await streamAIResponse(message, res);
  const end = performance.now();
  console.log(`⏱ Stream duration: ${(end - start).toFixed(2)} ms`);
});

export default router;

──────────────────────────────────────────────────────────────────────────────
4️⃣ Update src/app.ts
---------------------
Ensure your Express app correctly loads the chat router before starting the server.

import express from "express";
import cors from "cors";
import dotenv from "dotenv";
import chatRouter from "./routes/chat"; // ✅ Make sure this is imported at the top

dotenv.config();

const app = express();
const PORT = process.env.PORT || 3000;

app.use(cors());
app.use(express.json());

// ✅ Root test route
app.get("/", (req, res) => {
  res.send("🚀 AI Chat API is running!");
});

// ✅ Mount all chat routes
app.use("/api/chat", chatRouter);

// ✅ Start server
app.listen(PORT, () => {
  console.log(`✅ Server running on http://localhost:${PORT}`);
});

──────────────────────────────────────────────────────────────────────────────
5️⃣ Test Everything
-------------------
Run your dev server:
npm run dev

✅ Test standard mode:
curl -X POST http://localhost:3000/api/chat -H "Content-Type: application/json" -d "{"message":"Write a motivational quote."}"

Expected output:
{"reply":"\"Success is not the destination, but the journey of perseverance and passion that transforms every setback into a stepping stone.\""}  

✅ Test streaming mode:
curl -N -X POST http://localhost:3000/api/chat/stream -H "Content-Type: application/json" -d "{"message":"Write a one-sentence leadership quote."}"

Expected streaming output (token by token):
"True leadership is not about being in charge, but about inspiring others to rise and excel together."

──────────────────────────────────────────────────────────────────────────────
6️⃣ Confirm Logging and Observability
--------------------------------------
Console output example:
🧠 User message: Write a motivational quote.
⏱ Response time: 542.31 ms
⏱ Stream duration: 779.78 ms

✅ This confirms observability and performance logging are working.

──────────────────────────────────────────────────────────────────────────────
🎯 Outcome
Your API now supports:
• Real-time streaming responses
• Performance timing for every request
• Clean route structure and modular service layer
• Both synchronous and streaming AI endpoints

You now have a **production-grade AI backend** — exactly what employers expect from an AI Integration Engineer.

──────────────────────────────────────────────────────────────────────────────
End of Week 1 – Step 4 (Streaming & Observability)
